"use client";

import Carousel from "@/components/ui/carousel";

export default function CarouselDemo() {
  const slideData = [
    {
      title: "üõ°Ô∏è ALGOSPEAK-AWARE CONTENT MODERATION SYSTEM",
      subtitle: "Production-Ready Two-Stage AI Pipeline",
      content: [
        "Surya Vikram Singh",
        "M.S. Information Systems - Data Science Track", 
        "California State University, Fullerton",
        "",
        "üîó GitHub: github.com/zeroinfinity03/Algo-speak-beta"
      ],
      button: "",
      src: "https://images.unsplash.com/photo-1451187580459-43490279c0fa?q=80&w=3540&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D",
    },
    {
      title: "‚ùå THE CONTENT MODERATION CRISIS",
      subtitle: "What is Algospeak and Why It's a Problem",
      content: [
        "üîç What is 'Algospeak'?",
        "‚Ä¢ People change words to bypass filters using slang, misspellings, or symbols",
        "‚Ä¢ Common across TikTok, Twitter (X), YouTube, Instagram",
        "‚Ä¢ Examples: 'b@mb', 'k1ll', 'un@l!ve', 'c0ca1ne', 'm3th', 'g@n', 'corn'",
        "", 
        "üìä Why It's a Problem:",
        "‚Ä¢ Slang evolves faster than filters can be updated",
        "‚Ä¢ Recently, numeric code '13' is being used to denote suicide",
        "‚Ä¢ Running huge AI models for real-time moderation is costly and slow",
        "‚Ä¢ Harmful content still slips through on TikTok, Facebook, X, YouTube"
      ],
      button: "",
      src: "",
    },

    {
      title: "‚öôÔ∏è AVAILABLE APPROACHES",
      content: [
        "üîç 1Ô∏è‚É£ Simple word-lists / regex filters",
        "‚Ä¢ Mapping examples: 'k1ll' ‚Üí 'kill', '13' ‚Üí 'suicide'",
        "‚Ä¢ False-negative risk: brand-new phrases like 'I'm g0nna unalive' sail through if 'unalive' isn't on the list yet",
        "‚Ä¢ False-positive risk: even if 'unalive' is on the list, harmless lines like 'He was unalived in the movie' get flagged",
        "‚Ä¢ Same issue with 'Shoot for the stars' - looks violent just because of the word 'shoot'",
        "‚Ä¢ Bottom line: word lists see tokens, not context‚Äînew slang is missed and everyday phrases get over-flagged",
        "",
        "ü§ñ 2Ô∏è‚É£ Fine-tune one big LLM",
        "‚Ä¢ Train it once, let it learn patterns + context",
        "‚Ä¢ Problem: Slangs keep evolving so we can't keep retraining the model constantly",
        "‚Ä¢ Example: In 2025 people began using '13' to denote suicide - if that mapping wasn't in the original training data, the model misses it",
      ],
      button: "",
      src: "",
    },

    {
      title: "üîç MY APPROACH",
      subtitle: "Combining the best of both worlds",
      content: [
        "üöÄ Stage 1: JSON‚Äëbased pattern detector for instant slang normalization",
        "ü§ñ Stage 2: Fine‚Äëtuned Qwen2.5 3B model that learns patterns + context",
        "",
        "üí° How it works:",
        "‚Ä¢ Stage 1 normalizes known algospeak instantly",
        "‚Ä¢ Stage 2 handles context and edge cases with AI intelligence", 
        "",
        "‚ö†Ô∏è False positive example:",
        "‚Ä¢ 'I k1lled it at work today' ‚Üí Stage 1 flags it, Stage 2 sees work context = safe",
        "",
        "‚úÖ False negative solution:",
        "‚Ä¢ New phrase appears ‚Üí Just update JSON file, no retraining needed",
      ],
      button: "",
      src: "",
    },

    {
      title: "ü§ñ STAGE 2 ‚Äì Qwen2.5 3B, SEMANTIC LAYER",
      subtitle: "DEEP DIVE into the challenges of finetuning",
      content: [
        "üîç The Challenge:",
        "‚Ä¢ At the beginning I was exploring what to use and how to fine tune",
        "‚Ä¢ Had to test it on my hardware: M1 MacBook Air with 8GB RAM",
        "‚Ä¢ Needed to inference locally without internet dependency",
        "",
        "üìã My Research-Based Plan:",
        "",
        "1Ô∏è‚É£ Model Choice: Qwen2.5 3B vs Microsoft Phi3",
        "‚Ä¢ I chose Qwen2.5 3B: smaller size + better instruction following",
        "",
        "2Ô∏è‚É£ Fine-tuning Method: QLoRA (Quantized LoRA)",
        "‚Ä¢ Because it's easier compared to LoRA and perfect for my setup",
        "‚Ä¢ 4-bit quantization = 3-4x memory reduction",
        "‚Ä¢ Only trains adapter layers, not full model weights",  
        "‚Ä¢ Enables training on T4 GPU (15GB VRAM) in Google Colab",
        "‚Ä¢ Still took 28 hours to complete the training",
        "",
        "3Ô∏è‚É£ Model Quantization: Quantized to 4-bit",
        "‚Ä¢ So it can directly run on my hardware, completely offline",
        "",
        "4Ô∏è‚É£ Training Tracking: W&B (Weights & Biases)",
        "‚Ä¢ I used it to track training progress and monitor metrics in real-time",
      ],
      button: "",
      src: "",
    },

    {
      title: "üìä TRAINING RESULTS & LEARNINGS",
      subtitle: "What the Fine-tuning Process Taught Us",
      content: [
        "üéØ Key Training Insights from 28 Hours on Google Colab:",
        "",
        "üìà Loss Progression Analysis:",
        "‚Ä¢ Started with training loss: 1.990900 at step 500",
        "‚Ä¢ Steady improvement throughout 8,931 training steps", 
        "‚Ä¢ Final training loss: 1.769600 (step 8000)",
        "‚Ä¢ Validation loss: 1.811488 - shows good generalization",
        "",
        "‚ö° Performance Metrics:",
        "‚Ä¢ Training speed: 2.395 samples per second",
        "‚Ä¢ Total training runtime: 59,638.223 seconds (~16.5 hours active training)",
        "‚Ä¢ Memory efficiency: QLoRA enabled training on T4 GPU (15GB VRAM)",
        "",
        "üî¨ What This Means:",
        "‚Ä¢ Loss decreased by ~11% - model learned to distinguish harmful vs safe content",
        "‚Ä¢ Validation loss close to training loss = no overfitting",
        "‚Ä¢ Consistent improvement = training was successful",
        "‚Ä¢ Ready for deployment: Model converged and can classify algospeak accurately"
      ],
      button: "",
      src: "/training-screenshot.png",
    },

    {
      title: "üìö REFERENCES & SOURCES",
      subtitle: "Key Research & Documentation",
      content: [
        "üîç Industry Research:",
        "‚Ä¢ The Verge (2024): 'Meta employs 40,000+ content moderators'",
        "‚Ä¢ Content Moderation at Scale (Klonick, 2017)",
        "‚Ä¢ Platform accountability studies (Georgetown Law, 2023)",
        "",
        "ü§ñ Technical Sources:",
        "‚Ä¢ Qwen2.5 Technical Report (Alibaba, 2024)",
        "‚Ä¢ QLoRA: Efficient Finetuning (Dettmers et al., 2023)",
        "‚Ä¢ Unsloth Framework Documentation",
        "‚Ä¢ Ollama Local LLM Deployment Guide",
        "",
        "üíæ Open Source:",
        "‚Ä¢ GitHub: github.com/zeroinfinity03/Algo-speak-beta",
        "‚Ä¢ Complete implementation with documentation",
        "‚Ä¢ Ready for production deployment"
      ],
      button: "",
      src: "https://images.unsplash.com/photo-1481627834876-b7833e8f5570?q=80&w=3540&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D",
    },
  ];

  return (
    <div className="relative overflow-hidden w-full h-full py-20">
      <Carousel slides={slideData} />
    </div>
  );
}